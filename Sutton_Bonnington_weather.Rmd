---
title: "VC55 Weather Case Study"
author: Paul J. Palmer
output: 
  beamer_presentation:
    theme: "AnnArbor"
    colortheme: "dolphin"
    fonttheme: "structurebold"
    slide_level: 1
    includes:
          in_header: "preamble.tex" 
    keep_tex:  true
---

# Introduction

-   The purpose of this vignette is a practical demonstration of reusable templates based upon the novel concept of data state.
-   This report is used as a case study for the paper: *Achieving Analytical Fluency With Complex Data* and uses real world long term weather data as its source.
-   It is not the intention to analyse climate change, but the trends uncovered are striking, even in this single public domain source.

# Load Libraries

```{r child = "_header.Rmd"}
# Can I make comments here
```

```{r echo=FALSE}
# Set chunk visibility 
knitr::opts_chunk$set(echo = TRUE)

```

Load the Tidyverse libraries and other helpers before the analysis starts.

# Read The Sutton Bonnington Data

The source data is a text file that contains the following nominal data fields in an inconvenient format: YYYY, mm, tmax.degC, tmin.degC, airfrost.days, rain.mm, and sun.hours.

In addition we can also deduce the following fields using our understanding of what the data represents in the real world: place.name, latitude, longitude, height.amsl.metre

We use the function `get.weather.data.txt()` to encapsulate all the actions necessary to achieve this in a way that is independent of the actual data.

```{r echo=TRUE}
path.to.data <-
  fs::path("data-ext", "suttonboningtondata", ext = "txt")
suttonboningtondata <- get.weather.data.txt(path.to.data)
```

# Introducing Datum Triples

```{r FigTheoryNascentData, fig.cap=" Data representation", echo=FALSE}
path.to.image <- fs::path("images","standalone-data-definition", ext = "png")
knitr::include_graphics(path.to.image )
```

The datum triple (Entity; Attribute; Value) is the universal starting point for all data and requires the explicit use of a unique identifier for each row which we call `datumEntity`. When data is in a wide format, as at present, the identifier is implicit as in the row number, but this is a relative term that is affected by order. Note that this format makes no presumptions about number or types of attributes, and implicitly casts all the data in character format . There is no reason to keep any attribute with a value of `NA` as the row contains no information.

# File Contents

The first 12 rows of the `weather.data.triple` now look like this:

```{r ViewFileContents, echo=FALSE}

weather.data.triple <- data.as.triple(suttonboningtondata)

pander::pander(head(weather.data.triple,12))
```

# Saving The Raw Data

Finally we can save the data in the datum triple format for reuse. Although we have termed this as `data-raw`, multiple choices have been made in its transformation into this state, so is it really raw data? It should be clear that this `state` makes no assumptions about the data fields in any observation, so multiple files can be saved in the same directory as long as they use the same names for the three columns in the datum triple.

```{r SaveRawData }

save.data.as.triple(suttonboningtondata,"suttonboningtondata")

```

# Starting The Analysis With Data-Raw

For the purpose of this vignette we load the `data-raw` to demonstrate the the analysis could start with multiple files using the search style loading.

```{r LoadRawData}
# Load data-raw as datum triples
weather.data.triple <- load.data.raw("data-raw")

```

# Prepare `data-sl`

From the raw data we now prepare `data-sl` which is a loosely defined format of convenience. All the attributes are in character format, but may be in many different units. Re-arranging into a wider format is helpful as it is more human readable and easier to analyse.

```{r PrepareDataSL}
# Prepare data-sl
# First prepare the long data
VC55.weather.sl <- weather.data.triple %>%
                  pivot_wider(
  id_cols = datumEntity,
  names_from = datumAttribute,
  values_from = datumValue)
```

# Why Use Wide Format?

In this example we are interested in the weather through time so we select the columns of interest and make a long format with the date as the key column. At this point we can also lose any columns that are not required that may have been present if multiple sources of `data-raw` were used.

This wide format gives great flexibility with analysis and works well with a Grammar of Graphic (GoG) approach. This contrasts with the temptation to produce a compact summary of data as one might use in a spreadsheet analysis. The versatility of GoG will become apparent as we proceed and see how all plots may be specified by changing the GoG verbs.

# Create Data-ss

```{r CreateDataSS}
# But we are interesting in plotting the data by date so

  VC55.weather.ss <- VC55.weather.sl %>% 
    select("YYYYMMDD", 
           "tmax.degC",
           "tmin.degC",
           "airfrost.days", 
           "sun.hours" ,
           "rain.mm")     %>%
            pivot_longer(!YYYYMMDD,
                  names_to = "datumAttribute", 
                  values_to = "datumValue") 
```

# Specify Data types

At this point we can specify data types as Date and numeric for analysis.

```{r SpecifyDataTypes}
# Specify data types
VC55.weather.ss$YYYYMMDD <-  
  VC55.weather.ss$YYYYMMDD %>% as.Date
VC55.weather.ss$datumValue <- 
  VC55.weather.ss$datumValue %>% as.numeric()
```

# Save Data-ss

We can now save into `data-ss`.

```{r SaveDataSS}
dir.create( 
          fs::path( 
            "data-ss" ),  
            showWarnings = FALSE,
            recursive = TRUE) # Create the directory
#And save.
path.to.save.ss<- fs::path( 
                               "data-ss","VC55.weather.ss",
                            ext = "rds")
saveRDS(VC55.weather.ss, path.to.save.ss)

```

# Why Save Data-ss?

Once again the data may be loaded and the analysis start from this point.In this example the production of `data-ss` is quick, but for large an complex data we may wish to cache results for speedier analysis.

Rather than load as a named file, we demonstrate how multiple files in `data-ss` may be loaded and combined in a simple action. Since `data-ss` are strictly defined each file may be 'stacked' to combine into a larger data-set.

# Load Data-ss

```{r LoadDataSS}
# Load data-ss
path.to.data.ss <- fs::path("data-ss")

VC55.weather.ss <- list.files(
  path.to.data.ss,
  pattern = ".rds", # Make a suitable filter. 
  # Use the dot for a wildcard.
  full.names = TRUE,
  recursive = TRUE)  %>%
  purrr::map_df(readRDS) 

```


# Simple Check Plot

By faceting on `datumAttribute` we can produce a separate graph for each attribute. While is shows we have data, each graph has different units, so the scales do not make sense.

```{r SimpleCheckPlot, eval=TRUE, echo=FALSE}
data.to.plot <- VC55.weather.ss

data.to.plot %>% drop_na() %>%
ggplot(aes(colour = datumAttribute, 
           x= YYYYMMDD, 
           y =datumValue, 
           group = datumAttribute) ) + 
  geom_smooth( se = FALSE, method = "loess", 
               formula = "y ~ x") +
  facet_wrap(~ datumAttribute)
```

# Q-Q Plot Z-score Compared To Normal Distribution

Rather than 5 separate plots, if we use z-scores instead then each plot is normalised against zero and the standard deviation we can plot all the points on a common scale. The results seem to show that while a normal distribution is a reasonable model, this is not the case for extreme variances.

```{r CheckingDataDistributionQQ, eval=TRUE, echo=FALSE}
my.dist <- "norm"
data.to.plot <- VC55.weather.ss
 data.to.plot %>% drop_na() %>%
  group_by(datumAttribute) %>% 
  mutate( value = datumValue,
      Z.score = 
        (datumValue -mean(datumValue))/sd(datumValue)
        ) %>%
  ggplot(mapping = aes( 
                      group = datumAttribute,
                      sample = Z.score)) +
    stat_qq_line(distribution = my.dist) +
   stat_qq_point(distribution = my.dist) + 
  scale_colour_viridis_d() +
  ggtitle("Z-score compared to normal distribution") + 
   facet_wrap(~ datumAttribute)

```



# Normalising With Z-scores

Rather than 5 separate plots, if we use z-scores instead then each plot is normalised against zero and the standard deviation we can plot all the points on a common scale. However, the result is hard to interpret because the variation between points is large.

```{r PointsWithZScoreCode, echo =FALSE}

data.to.plot <- VC55.weather.ss
 data.to.plot %>% drop_na() %>%
  group_by(datumAttribute) %>% 
  mutate( value = datumValue,
  Z.score = 
    (datumValue -mean(datumValue))/sd(datumValue)
        ) %>%
  ggplot( aes(colour = datumAttribute, 
              x= YYYYMMDD, y =Z.score, 
              group = datumAttribute) ) + 
              geom_point() +
              scale_colour_viridis_d()
```


# Trends in With Z-scores

However, if try to fit a local trend (Loess curve), then it is easier to visualise trends.


```{r NormalisingWithZScoreCode, echo = FALSE}

data.to.plot <- VC55.weather.ss
 data.to.plot %>% drop_na() %>%
  group_by(datumAttribute) %>% 
  mutate( value = datumValue,
  Z.score = 
    (datumValue -mean(datumValue))/sd(datumValue)
        ) %>%
  ggplot( aes(colour = datumAttribute, 
              x= YYYYMMDD, y =Z.score, 
              group = datumAttribute) ) + 
              geom_smooth( se = FALSE, 
                           method = "loess", 
                           formula = "y ~ x") + 
              scale_colour_viridis_d()
```


# Assumptions

Our assumption or a normal distribution is reasonably valid, but not perfect. As we are looking at weather data, it might be nice to look at: Annual Total Rainfall, Annual Air Frost days, Annual Maximum Temperature, and Annual Minimum Temperature. Again GoG comes to the rescue and we can quickly produce the following plots.

# Weather Plots In Native Units

For convenience, the plot routine has been written as a function so we can get all the plots quickly by reusing the code and just changing the selection attribute.

```{r PlotsInNativeUnits}
ggfun <- function(dat, nice.title){
  plot.output <- ggplot(data = dat,
                aes(x = YYYY,
                    y = annual)) +
    geom_point() +
    geom_smooth( se = FALSE, 
             method = "lm", formula = "y ~ x") +
  geom_line()+ ggtitle( nice.title ) + ylab(nice.title)
  return(plot.output)
}
```

# Filter The Data

```{r FilterTheData}
# Weather "tmax.degC"     "tmin.degC"    
# "airfrost.days" "sun.hours"     "rain.mm"
data.to.plot.local <- VC55.weather.ss[
  VC55.weather.ss$datumAttribute == "airfrost.days", ]
data.to.plot.local$YYYY <- format(
  data.to.plot.local$YYYYMMDD, format = "%Y") %>% 
    as.numeric()
data.to.plot.annual <- data.to.plot.local %>% 
  drop_na() %>% group_by(YYYY) %>%
    mutate( 
  # Change to max, min etc. as required.
          annual = sum(datumValue)
        ) 
```

# Plot Total Airfrost Days

```{r PlotAirFrost, echo=FALSE}
data.to.plot.local <- VC55.weather.ss[VC55.weather.ss$datumAttribute == "airfrost.days", ]
data.to.plot.local$YYYY <- format(data.to.plot.local$YYYYMMDD, format = "%Y") %>% as.numeric()

data.to.plot.annual <- data.to.plot.local %>% drop_na() %>%
    group_by(YYYY) %>%
    mutate( 
          annual = sum(datumValue)
        ) 

ggfun(dat = data.to.plot.annual, nice.title = "Total Air Frost Days" ) 

#weather.plots.list$`max temp degC`

```

# Plot Maximum Temperature

```{r PlotMaxTemp, echo=FALSE}
data.to.plot.local <- VC55.weather.ss[VC55.weather.ss$datumAttribute == "tmax.degC", ]
data.to.plot.local$YYYY <- format(data.to.plot.local$YYYYMMDD, format = "%Y") %>% as.numeric()

data.to.plot.annual <- data.to.plot.local %>% drop_na() %>%
    group_by(YYYY) %>%
    mutate( 
          annual = max(datumValue)
        ) 

ggfun(dat = data.to.plot.annual, nice.title = "Maximum Temperature" ) 

```

# Plot Minimum Temperature

```{r PlotMinTemp, echo=FALSE}
data.to.plot.local <- VC55.weather.ss[VC55.weather.ss$datumAttribute == "tmin.degC", ]
data.to.plot.local$YYYY <- format(data.to.plot.local$YYYYMMDD, format = "%Y") %>% as.numeric()

data.to.plot.annual <- data.to.plot.local %>% drop_na() %>%
    group_by(YYYY) %>%
    mutate( 
          annual = min(datumValue)
        ) 

ggfun(dat = data.to.plot.annual, nice.title = "Mainimum Temperature" ) 

```

# Plot Total Rainfail

```{r PlotTotalRainfall, echo=FALSE}
data.to.plot.local <- VC55.weather.ss[VC55.weather.ss$datumAttribute == "rain.mm", ]
data.to.plot.local$YYYY <- format(data.to.plot.local$YYYYMMDD, format = "%Y") %>% as.numeric()

data.to.plot.annual <- data.to.plot.local %>% drop_na() %>%
    group_by(YYYY) %>%
    mutate( 
          annual = sum(datumValue)
        ) 

ggfun(dat = data.to.plot.annual, nice.title = "Annual Total Rainfall mm" ) 

```

# Conclusion

This vignette demonstrates the versatility of using data state in conjunction with GoG.
