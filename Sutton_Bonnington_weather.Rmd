---
title: "VC55 Weather Case Study"
author: Paul J. Palmer
output: 
  beamer_presentation:
    theme: "AnnArbor"
    colortheme: "dolphin"
    fonttheme: "structurebold"
    slide_level: 1
    includes:
          in_header: "preamble.tex" 
    keep_tex:  true
---
```{r echo=FALSE}
# Set chunk visibility 
see.chunk <- FALSE
```


# Introduction

-   The purpose of this vignette is a practical demonstration of reusable templates based upon the novel concept of data state.
-   This report is used as a case study for the paper: *Achieving Analytical Fluency With Complex Data* and uses real world long term weather data as its source.
-   It is not the intention to analyse climate change, but the trends uncovered are striking, even in this single public domain source.

# Load Libraries

```{r child = "_header.Rmd"}

```


Load the Tidyverse libraries and other helpers before the analysis starts.


# Read The Sutton Bonnington Data

The source data is a text file that contains the following nominal data fields in an inconvenient format: YYYY, mm, tmax.degC, tmin.degC, airfrost.days, rain.mm, and sun.hours.

In addition we can also deduce the following fields using our understanding of what the data represents in the real world:
place.name, latitude, longitude, height.amsl.metre

We use the function `get.weather.data.txt()` to encapsulate all the actions necessary to achieve this in a way that is independent of the actual data.


```{r echo=TRUE}
path.to.data <-
  fs::path("data-ext", "suttonboningtondata", ext = "txt")
suttonboningtondata <- get.weather.data.txt(path.to.data)
```


# Introducing Datum Triples

The long datum triple format requires the explicit use of a unique identifier for each row which we call `datumEntity`. When data is in a wide format, as at present, the identifier is implicit as in the row number, but this is a relative term that is affected by order. The analysis that follows uses the advantages that are brought with an explicit definition.


# Working With Datum Triples

The datum triple is the universal starting point for all data. Converting to this format allows us to combine data from multiple sources, as long as the `datumEntity` only ever refers to a unique observation. Note that this format makes no presumptions about attributes, but it does require all data to be represented as characters. There is no reason to keep any attribute with a value of `NA` as the row contains no information.

# Saving The Raw Data

Finally we can save the data in a machine readable format for reuse. Although we have termed this as `data-raw`, multiple choices have been made in its transformation into this state. It should be clear that this `state` makes no assumptions about numbers of fields in any observation.

```{r SaveRawData }

save.raw.data (suttonboningtondata,"suttonboningtondata")

```

# Starting The Analysis With Data-Raw

For the purpose of this vignette we load the `data-raw` to demonstrate the the analysis could start with multiple files using the search style loading.

```{r LoadRawData}
# Load data-raw
path.to.data <- fs::path( 
                        "data-raw")
weather.data.triple <- list.files(
  path.to.data,
  pattern = ".rds$", # Make a suitable filter.
  full.names = TRUE,
  recursive = TRUE) %>%
   purrr::map_df(readRDS) %>%
   rbind()
```

# File Contents

The first 13 rows of the `weather.data.triple` now look like this:

```{r ViewFileContents, echo=FALSE}
pander::pander(head(weather.data.triple,13))
```

# Prepare `data-sl`

From the raw data we now prepare `data-sl` which is a loosely defined format of convenience. All the attributes are in character format, but may be in many different units. Re-arranging into a wider format is helpful

```{r PrepareDataSL}
# Prepare data-sl
# First prepare the long data
VC55.weather.sl <- weather.data.triple %>%
                  pivot_wider(
  id_cols = datumEntity,
  names_from = datumAttribute,
  values_from = datumValue)
```

# Why Use Wide Format?

To prepare strictly defined data to analyse the weather we select the columns of interest and make a long format with the date as the key column. At this point we loose any columns that are not required that may have be present if multiple sources of `data-raw` were used.

We choose this format as it gives great flexibility with analysis and works well with a Grammar of Graphic (GoG) approach. This contrasts with the temptation to produce a wide format of data as one might use in a spreadsheet analysis. The versatility of GoG will become apparent as we proceed and see how all plots may be specified by changing the GoG verbs.

# Create Data-ss

```{r CreateDataSS}
# But we are interesting in plotting the data by date so

  VC55.weather.ss <- VC55.weather.sl %>% 
    select("YYYYMMDD", 
           "tmax.degC",
           "tmin.degC",
           "airfrost.days", 
           "sun.hours" ,
           "rain.mm")     %>%
            pivot_longer(!YYYYMMDD,
                  names_to = "datumAttribute", 
                  values_to = "datumValue") 
```

# Specify Data types

At this point we can specify data types as Date and numeric for analysis.

```{r SpecifyDataTypes}
# Specify data types
VC55.weather.ss$YYYYMMDD <-  
  VC55.weather.ss$YYYYMMDD %>% as.Date
VC55.weather.ss$datumValue <- 
  VC55.weather.ss$datumValue %>% as.numeric()
```

# Save Data-ss

We can now save into `data-ss`.

```{r SaveDataSS}
dir.create( 
          fs::path( 
            "data-ss" ),  
            showWarnings = FALSE,
            recursive = TRUE) # Create the directory
#And save.
path.to.save.ss<- fs::path( 
                               "data-ss","VC55.weather.ss",
                            ext = "rds")
saveRDS(VC55.weather.ss, path.to.save.ss)

```

# Why Save Data-ss?

Once again the data may be loaded and the analysis start from this point. Rather than load as a named file, we demonstrate how multiple files in `data-ss` may be loaded and combined in a simple action. Since `data-ss` are strictly defined each file may be 'stacked' to combine into a larger data-set. If this were really the case then it would make sense to rename the data to something more appropriate. The named method of loading is included as comments for comparison.

# Load Data-ss

```{r LoadDataSS}
# Load data-ss
path.to.data.ss <- fs::path("data-ss")

VC55.weather.ss <- list.files(
  path.to.data.ss,
  pattern = ".rds", # Make a suitable filter. 
  # Use the dot for a wildcard.
  full.names = TRUE,
  recursive = TRUE)  %>%
  purrr::map_df(readRDS) 

```

# A Simple Check Plot

By faceting on `datumAttribute` we can produce a separate graph for each attribute. While is shows we have data, each graph has different units, so the scales do not make sense.

```{r SimpleCheckPlotCode, eval=FALSE}
data.to.plot <- VC55.weather.ss
data.to.plot %>% drop_na() %>%
ggplot(aes(colour = datumAttribute, 
           x= YYYYMMDD, 
           y =datumValue, 
           group = datumAttribute) ) + 
 # geom_point() +
  geom_smooth( se = FALSE, 
     method = "loess", formula = "y ~ x") +
  facet_wrap(~ datumAttribute)
```

# Simple Check Plot

By faceting on `datumAttribute` we can produce a separate graph for each attribute. While is shows we have data, each graph has different units, so the scales do not make sense.

```{r SimpleCheckPlot, eval=TRUE, echo=FALSE}
data.to.plot <- VC55.weather.ss

data.to.plot %>% drop_na() %>%
ggplot(aes(colour = datumAttribute, 
           x= YYYYMMDD, 
           y =datumValue, 
           group = datumAttribute) ) + 
  geom_smooth( se = FALSE, method = "loess", 
               formula = "y ~ x") +
  facet_wrap(~ datumAttribute)
```

# Normalising With Z-scores

However, if we use z-scores instead then each plot is normalised against zero and the standard deviation.

```{r NormalisingWithZScoreCode, eval=FALSE}

data.to.plot <- VC55.weather.ss
 data.to.plot %>% drop_na() %>%
  group_by(datumAttribute) %>% 
  mutate( value = datumValue,
  Z.score = 
    (datumValue -mean(datumValue))/sd(datumValue)
        ) %>%
  ggplot( aes(colour = datumAttribute, 
              x= YYYYMMDD, y =Z.score, 
              group = datumAttribute) ) + 
              geom_smooth( se = FALSE, 
                           method = "loess", 
                           formula = "y ~ x") + 
              scale_colour_viridis_d()
```

# Plotting With Z-scores

Rather than 5 separate plots, we can use a single graph to plot a smooth loess regression for each attribute.

```{r NormalisingWithZScore, eval= TRUE, echo=FALSE}

data.to.plot <- VC55.weather.ss
data.to.plot %>% drop_na() %>% group_by(datumAttribute) %>% 
  mutate( value = datumValue, Z.score = 
        (datumValue -mean(datumValue))/sd(datumValue)) %>%
  ggplot( aes(colour = datumAttribute, 
      x= YYYYMMDD, y =Z.score, 
      group = datumAttribute) ) + 
      geom_smooth( se = FALSE, 
      method = "loess",  formula = "y ~ x") + 
      scale_colour_viridis_d()
```

# Checking Data Distribution

```{r CheckingDataDistributionCode, eval=FALSE}
my.dist <- "norm"
data.to.plot <- VC55.weather.ss
 data.to.plot %>% drop_na() %>%
  group_by(datumAttribute) %>% 
  mutate( value = datumValue,
      Z.score = 
        (datumValue -mean(datumValue))/sd(datumValue)
        ) %>%
  ggplot(mapping = aes( 
                      group = datumAttribute,
                      sample = Z.score)) +
    stat_qq_line(distribution = my.dist) +
   stat_qq_point(distribution = my.dist) + 
  scale_colour_viridis_d() +
  ggtitle("Z-score compared to normal distribution") + 
   facet_wrap(~ datumAttribute)
  

```

# Q-Q Plot Z-score Compared To Normal Distribution

This looks good but it is predicated upon the deviations being normally distributed about the mean.

```{r CheckingDataDistributionQQ, eval=TRUE, echo=FALSE}
my.dist <- "norm"
data.to.plot <- VC55.weather.ss
 data.to.plot %>% drop_na() %>%
  group_by(datumAttribute) %>% 
  mutate( value = datumValue,
      Z.score = 
        (datumValue -mean(datumValue))/sd(datumValue)
        ) %>%
  ggplot(mapping = aes( 
                      group = datumAttribute,
                      sample = Z.score)) +
    stat_qq_line(distribution = my.dist) +
   stat_qq_point(distribution = my.dist) + 
  scale_colour_viridis_d() +
  ggtitle("Z-score compared to normal distribution") + 
   facet_wrap(~ datumAttribute)

```

# Assumptions

Our assumption or a normal distribution is reasonably valid, but not perfect. As we are looking at weather data, it might be nice to look at: Annual Total Rainfall, Annual Air Frost days, Annual Maximum Temperature, and Annual Minimum Temperature. Again GoG comes to the rescue and we can quickly produce the following plots.

# Weather Plots In Native Units

For convenience, the plot routine has been written as a function so we can get all the plots quickly by reusing the code and just changing the selection attribute.

```{r PlotsInNativeUnits}
ggfun <- function(dat, nice.title){
  plot.output <- ggplot(data = dat,
                aes(x = YYYY,
                    y = annual)) +
    geom_point() +
    geom_smooth( se = FALSE, 
             method = "lm", formula = "y ~ x") +
  geom_line()+ ggtitle( nice.title ) + ylab(nice.title)
  return(plot.output)
}
```

# Filter The Data

```{r FilterTheData}
# Weather "tmax.degC"     "tmin.degC"    
# "airfrost.days" "sun.hours"     "rain.mm"
data.to.plot.local <- VC55.weather.ss[
  VC55.weather.ss$datumAttribute == "airfrost.days", ]
data.to.plot.local$YYYY <- format(
  data.to.plot.local$YYYYMMDD, format = "%Y") %>% 
    as.numeric()
data.to.plot.annual <- data.to.plot.local %>% 
  drop_na() %>% group_by(YYYY) %>%
    mutate( 
  # Change to max, min etc. as required.
          annual = sum(datumValue)
        ) 
```

# Plot Total Airfrost Days

```{r PlotAirFrost, echo=FALSE}
data.to.plot.local <- VC55.weather.ss[VC55.weather.ss$datumAttribute == "airfrost.days", ]
data.to.plot.local$YYYY <- format(data.to.plot.local$YYYYMMDD, format = "%Y") %>% as.numeric()

data.to.plot.annual <- data.to.plot.local %>% drop_na() %>%
    group_by(YYYY) %>%
    mutate( 
          annual = sum(datumValue)
        ) 

ggfun(dat = data.to.plot.annual, nice.title = "Total Air Frost Days" ) 

#weather.plots.list$`max temp degC`

```

# Plot Maximum Temperature

```{r PlotMaxTemp, echo=FALSE}
data.to.plot.local <- VC55.weather.ss[VC55.weather.ss$datumAttribute == "tmax.degC", ]
data.to.plot.local$YYYY <- format(data.to.plot.local$YYYYMMDD, format = "%Y") %>% as.numeric()

data.to.plot.annual <- data.to.plot.local %>% drop_na() %>%
    group_by(YYYY) %>%
    mutate( 
          annual = max(datumValue)
        ) 

ggfun(dat = data.to.plot.annual, nice.title = "Maximum Temperature" ) 

```

# Plot Minimum Temperature

```{r PlotMinTemp, echo=FALSE}
data.to.plot.local <- VC55.weather.ss[VC55.weather.ss$datumAttribute == "tmin.degC", ]
data.to.plot.local$YYYY <- format(data.to.plot.local$YYYYMMDD, format = "%Y") %>% as.numeric()

data.to.plot.annual <- data.to.plot.local %>% drop_na() %>%
    group_by(YYYY) %>%
    mutate( 
          annual = min(datumValue)
        ) 

ggfun(dat = data.to.plot.annual, nice.title = "Mainimum Temperature" ) 

```

# Plot Total Rainfail

```{r PlotTotalRainfall, echo=FALSE}
data.to.plot.local <- VC55.weather.ss[VC55.weather.ss$datumAttribute == "rain.mm", ]
data.to.plot.local$YYYY <- format(data.to.plot.local$YYYYMMDD, format = "%Y") %>% as.numeric()

data.to.plot.annual <- data.to.plot.local %>% drop_na() %>%
    group_by(YYYY) %>%
    mutate( 
          annual = sum(datumValue)
        ) 

ggfun(dat = data.to.plot.annual, nice.title = "Annual Total Rainfall mm" ) 

```

# Conclusion

This vignette demonstrates the versatility of using data state in conjunction with GoG.
