---
title: "VC55 Weather Case Study"
author: Paul J. Palmer
output: 
  beamer_presentation:
    theme: "AnnArbor"
    colortheme: "dolphin"
    fonttheme: "structurebold"
    slide_level: 1
    includes:
          in_header: "preamble.tex" 

---


```{r setDefaults,  include=TRUE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, results='asis'}
# Set code chunk options for all chunks
# These can be overridden at the chunk level, but setting global options 
# ensures consistency of chunk behaviour.
# To print a version of this document without code set echo = FALSE. 
#Include false will ignore the code chunk completely!
knitr::opts_chunk$set(include = TRUE, 
                      echo = TRUE, 
                      warning=FALSE, 
                      message=FALSE, 
                      error=FALSE, 
                     # fig.width = 3,
                    #  fig.height = 2,
                     # fig.width = 3,
                    out.width = '70%',
                      fig.align = "center",
                      results='asis')



```

# Introduction

  - The purpose of this vignette is a practical demonstration of reusable templates based upon the novel concept of data state. 
  - This report is used as a case study for the paper: *Achieving Analytical Fluency With Complex Data* and uses real world long term weather data as its source.
  - It is not the intention to analyse climate change, but the trends uncovered are striking, even in this single public domain source.


# Load Libraries

Load the Tidyverse libraries and other helpers before the analysis starts.

```{r }
# Load libraries
library(tidyverse) # Load all the Tidyverse packages in one go.
library(kableExtra) # Enable advanced table styling
library(qqplotr) # Enable QQ plots for statistical analysis
library(readr) # Improved reading of text files
library(timeDate) # Helper for time date manipulation
library(rprojroot) # Useful utilities
library(fs) # System independent file paths

```

# Read The Sutton Bonnington Data

The absolute path helps to write code that is computer independent but it does use the Rstudio specific function to find the current project location. The use of `fs` to build the file path ensures compatibility across all operating systems. It also gives a chance to check that it is correct before loading the file. Note that the use of `rprojroot::` and `fs::` is not strictly necessary as the library has been loaded. This form explicitly specifies the installed package from which the functions are called and is used here for clarity.

# Define The Data Path
```{r }
# Read data
absolute.path <- rprojroot::find_rstudio_root_file()
path.to.my.data <- fs::path( absolute.path,
                             "data-ext",
                             "suttonboningtondata", 
                            ext = "txt")
```

# Read the Data
The `read_table` function does a good job of reading the data into columns recognising the partitioning of fields with the use of spaces after the first five lines of text are skipped. Although there are numerous **warnings** comparing those rows with the final output (allowing for the 5 row offset), uncovers no **errors**. There are 7 data columns used and the warnings are generated by occasional characters in column 8 which we ignore. The investigation of such anomalies is an important part of the data reading process, and while we do not need to know why they occur, their interpretation affects the onward analysis.

```{r }
suttonboningtondata <- read_table(
  path.to.my.data,
  skip = 5) # skip 5 lines of text
```

# Rework The Data
Renaming the column names requires checking the results manually with `view(suttonboningtondata)` before making a list of names. 

```{r }
# Rename the columns to something useful
new.colnames <- c( "YYYY",
                   "mm",
                   "tmax.degC",
                   "tmin.degC",
                   "airfrost.days",
                   "rain.mm",
                   "sun.hours"
  )
colnames(suttonboningtondata) <- new.colnames
```
The use of spaces has been avoided since they can cause problems in programmatic analysis.

# Delete Spurious Rows
The text file headings were split across two rows so the first row of the data contains fragments which we can drop since we have included them in our column names manually.

```{r }
# Drop row 1
suttonboningtondata <- suttonboningtondata[-1,]
```

# Remove Unwanted Characters
We can now remove the unwanted characters that remain in the data. Note the use of the double escape `\\*` to select the asterisk. The removal of unwanted special characters is often a problem in nascent data so it is best to get rid of them before they cause problems when code is executed. Not how `tidyverse`\` syntax allows us to daisy chain the substitutions,

```{r }
suttonboningtondata <- suttonboningtondata %>%
                        map_df( gsub,
                                pattern = "\\*",
                                replacement = "") %>%
                        map_df( gsub,
                                pattern = "---",
                                replacement = NA
                                )
```

# Problems With Dates
The early months of the year are represented by a single digit so we need to pad them with a leading zero. The we can create a date in the ISO recognised format: `YYYY-MM-DD`. As these are monthly summaries we choose the last day of the month which allows us to convert from a character string to a date when we plot results. We do this by building a dummy. but correctly formatted date, and then finding the last day of the month using the helper function from `timeDate`. There are numerous in which this could have been calculated, but the R language users have written many helpers for tedious tasks such as this.

# Building The Nominal Dates
```{r }
suttonboningtondata$mm <-   stringr::str_pad(
  suttonboningtondata$mm, width = 2, pad = "0")
 # Create a date at the end of the month.
dummyDate <- with( suttonboningtondata, 
                  paste( YYYY, 
                  mm, 
                  "28", # Need a dummy day
                  sep= "-"   ) 
                  )
# Calculates the last day of the month
 suttonboningtondata$YYYYMMDD <- 
   timeDate::timeLastDayInMonth(dummyDate) %>% 
   as.character()
```

# Introducing Datum Triples

The long datum triple format requires the explicit use of a unique identifier for each row which we call `datumEntity`. When data is in a wide format, as at present, the identifier is implicit as in the row number, but this is a relative term that is affected by order. The analysis that follows uses the advantages that are brought with an explicit definition.

```{r }
suttonboningtondata$datumEntity <- with(
  suttonboningtondata,
  paste("Sutton.Bonnington", YYYY, mm, sep = ":" )
  )
```

# Adding Nominal Attributes
There are also some data attributes which were described in the five lines of text which apply to all rows of data. We manually add them here as they will be useful if we combine our data with similar sources from other locations.

```{r }
# manually name some values
suttonboningtondata$place <- "Sutton Bonnington"
suttonboningtondata$lattitude <- 52.833
suttonboningtondata$logitude <- 52.833
suttonboningtondata$logitude <- -1.250
suttonboningtondata$easting <- 450700
suttonboningtondata$northing <- 325900
suttonboningtondata$height.amsl.metre <- 48
```

# Working With Datum Triples
The datum triple is the universal starting point for all data. Converting to this format allows us to combine data from multiple sources, as long as the `datumEntity` only ever refers to a unique observation. Note that this format makes no presumptions about attributes, but it does require all data to be represented as characters. There is no reason to keep any attribute with a value of `NA` as the row contains no information.

```{r }
suttonbonington.triple <- suttonboningtondata %>% 
  map_df(as.character) %>% # Convert to character
  pivot_longer(!datumEntity, # datumEntity as  key
  names_to = "datumAttribute", # Attribute name
  values_to = "datumValue" # Save  value
      ) %>% drop_na() # Drop NAs
```

# Saving The Raw Data
Finally we can save the data in a machine readable format for reuse. Although we have termed this as `data-raw`, multiple choices have been made in its transformation into this state. It should be clear that this `state` makes no assumptions about numbers of fields in any observation.

```{r }
dir.create( 
          fs::path( absolute.path,
            "data-raw" ),  
            showWarnings = FALSE,
            recursive = TRUE) # 

path.to.save<- fs::path( absolute.path,
 "data-raw","suttonboningtondata", ext = "rds")
saveRDS(suttonbonington.triple,path.to.save)


```

# Starting The Analysis With Data-Raw
For the purpose of this vignette we load the `data-raw` to demonstrate the the analysis could start with multiple files using the search style loading. 

```{r}
# Load data-raw
path.to.data <- fs::path( absolute.path
                               ,"data-raw")
weather.data.triple <- list.files(
  path.to.data,
  pattern = ".rds$", # Make a suitable filter.
  full.names = TRUE,
  recursive = TRUE) %>%
   purrr::map_df(readRDS) %>%
   rbind()
```

# File Contents
The first 13 rows of the `weather.data.triple` now look like this:

```{r echo=FALSE}
pander::pander(head(weather.data.triple,13))
```

# Prepare `data-sl`
From the raw data we now prepare `data-sl` which is a loosely defined format of convenience. All the attributes are in character format, but may be in many different units. Re-arranging into a wider format is helpful

```{r}
# Prepare data-sl
# First prepare the long data
VC55.weather.sl <- weather.data.triple %>%
                  pivot_wider(
  id_cols = datumEntity,
  names_from = datumAttribute,
  values_from = datumValue)
```

# Why Use Wide Format?
To prepare strictly defined data to analyse the weather we select the columns of interest and make a long format with the date as the key column. At this point we loose any columns that are not required that may have be present if multiple sources of `data-raw` were used.

We choose this format as it gives great flexibility with analysis and works well with a Grammar of Graphic (GoG) approach. This contrasts with the temptation to produce a wide format of data as one might use in a spreadsheet analysis. The versatility of GoG will become apparent as we proceed and see how all plots may be specified by changing the GoG verbs.

# Create Data-ss
```{r}
# But we are interesting in plotting the data by date so

  VC55.weather.ss <- VC55.weather.sl %>% 
    select("YYYYMMDD", 
           "tmax.degC",
           "tmin.degC",
           "airfrost.days", 
           "sun.hours" ,
           "rain.mm")     %>%
            pivot_longer(!YYYYMMDD,
                  names_to = "datumAttribute", 
                  values_to = "datumValue") 
```

# Specify Data types
At this point we can specify data types as Date and numeric for analysis.

```{r}
# Set the data types
VC55.weather.ss$YYYYMMDD <-  VC55.weather.ss$YYYYMMDD %>% as.Date
VC55.weather.ss$datumValue <- VC55.weather.ss$datumValue %>% as.numeric()
```

# Save Data-ss
We can now save into `data-ss`.

```{r}
dir.create( 
          fs::path( absolute.path,
            "data-ss" ),  
            showWarnings = FALSE,
            recursive = TRUE) # Create the directory

#And save.
path.to.save.ss<- fs::path( absolute.path
                               ,"data-ss","VC55.weather.ss", ext = "rds")
saveRDS(VC55.weather.ss, path.to.save.ss)

```

# Why Save Data-ss?
Once again the data may be loaded and the analysis start from this point. Rather than load as a named file, we demonstrate how multiple files in `data-ss` may be loaded and combined in a simple action. Since `data-ss` are strictly defined each file may be 'stacked' to combine into a larger data-set. If this were really the case then it would make sense to rename the data to something more appropriate. The named method of loading is included as comments for comparison.

# Load Data-ss
```{r}
# Load data-ss
path.to.data.ss <- fs::path(absolute.path,"data-ss")

VC55.weather.ss <- list.files(
  path.to.data.ss,
  pattern = ".rds", # Make a suitable filter. Use the dot for a wildcard.
  full.names = TRUE,
  recursive = TRUE)  %>%
  purrr::map_df(readRDS) 


```

# A Simple Check Plot
By faceting on `datumAttribute` we can produce a separate graph for each attribute. While is shows we have data, each graph has different units, so the scales do not make sense.

```{r eval=FALSE}
data.to.plot <- VC55.weather.ss
data.to.plot %>% drop_na() %>%
ggplot(aes(colour = datumAttribute, 
           x= YYYYMMDD, 
           y =datumValue, 
           group = datumAttribute) ) + 
 # geom_point() +
  geom_smooth( se = FALSE, 
     method = "loess", formula = "y ~ x") +
  facet_wrap(~ datumAttribute)
```


# Simple Check Plot
By faceting on `datumAttribute` we can produce a separate graph for each attribute. While is shows we have data, each graph has different units, so the scales do not make sense.

```{r eval=TRUE, echo=FALSE}
data.to.plot <- VC55.weather.ss

data.to.plot %>% drop_na() %>%
ggplot(aes(colour = datumAttribute, 
           x= YYYYMMDD, 
           y =datumValue, 
           group = datumAttribute) ) + 
  geom_smooth( se = FALSE, method = "loess", 
               formula = "y ~ x") +
  facet_wrap(~ datumAttribute)
```

# Normalising With Z-scores
However, if we use z-scores instead then each plot is normalised against zero and the standard deviation. 

```{r eval=FALSE}

data.to.plot <- VC55.weather.ss
 data.to.plot %>% drop_na() %>%
  group_by(datumAttribute) %>% 
  mutate( value = datumValue,
  Z.score = 
    (datumValue -mean(datumValue))/sd(datumValue)
        ) %>%
  ggplot( aes(colour = datumAttribute, 
              x= YYYYMMDD, y =Z.score, 
              group = datumAttribute) ) + 
              geom_smooth( se = FALSE, 
                           method = "loess", 
                           formula = "y ~ x") + 
              scale_colour_viridis_d()
```

# Plotting With Z-scores
Rather than 5 separate plots, we can use a single graph to plot a smooth loess regression for each attribute.
```{r eval= TRUE, echo=FALSE}

data.to.plot <- VC55.weather.ss
 data.to.plot %>% drop_na() %>%
  group_by(datumAttribute) %>% 
  mutate( value = datumValue,
      Z.score = (datumValue -mean(datumValue))/sd(datumValue)
        ) %>%
  ggplot( aes(colour = datumAttribute, 
      x= YYYYMMDD, y =Z.score, 
      group = datumAttribute) ) + 
      geom_smooth( se = FALSE, 
                   method = "loess", 
                   formula = "y ~ x") + 
              scale_colour_viridis_d()

```


# Checking Data Distribution

```{r eval=FALSE}
my.dist <- "norm"
data.to.plot <- VC55.weather.ss
 data.to.plot %>% drop_na() %>%
  group_by(datumAttribute) %>% 
  mutate( value = datumValue,
      Z.score = 
        (datumValue -mean(datumValue))/sd(datumValue)
        ) %>%
  ggplot(mapping = aes( 
                      group = datumAttribute,
                      sample = Z.score)) +
    stat_qq_line(distribution = my.dist) +
   stat_qq_point(distribution = my.dist) + 
  scale_colour_viridis_d() +
  ggtitle("Z-score compared to normal distribution") + 
   facet_wrap(~ datumAttribute)
  

```

# Q-Q Plot Z-score Compared To Normal Distribution
This looks good but it is predicated upon the deviations being normally distributed about the mean. 
```{r eval=TRUE, echo=FALSE}
my.dist <- "norm"
data.to.plot <- VC55.weather.ss
 data.to.plot %>% drop_na() %>%
  group_by(datumAttribute) %>% 
  mutate( value = datumValue,
      Z.score = 
        (datumValue -mean(datumValue))/sd(datumValue)
        ) %>%
  ggplot(mapping = aes( 
                      group = datumAttribute,
                      sample = Z.score)) +
    stat_qq_line(distribution = my.dist) +
   stat_qq_point(distribution = my.dist) + 
  scale_colour_viridis_d() +
  ggtitle("Z-score compared to normal distribution") + 
   facet_wrap(~ datumAttribute)

```

# Assumptions
Our assumption or a normal distribution is reasonably valid, but not perfect. As we are looking at weather data, it might be nice to look at: Annual Total Rainfall, Annual Air Frost days, Annual Maximum Temperature, and Annual Minimum Temperature. Again GoG comes to the rescue and we can quickly produce the following plots.

# Weather Plots In Native Units
For convenience, the plot routine has been written as a function so we can get all the plots quickly by reusing the code and just changing the selection attribute.

```{r}
ggfun <- function(dat, nice.title){
  plot.output <- ggplot(data = dat,
                aes(x = YYYY,
                    y = annual)) +
    geom_point() +
    geom_smooth( se = FALSE, 
             method = "lm", formula = "y ~ x") +
  geom_line()+ ggtitle( nice.title ) + ylab(nice.title)
  return(plot.output)
}
```



# Filter The Data

```{r}
# Weather "tmax.degC"     "tmin.degC"    
# "airfrost.days" "sun.hours"     "rain.mm"
data.to.plot.local <- VC55.weather.ss[
  VC55.weather.ss$datumAttribute == "airfrost.days", ]
data.to.plot.local$YYYY <- format(
  data.to.plot.local$YYYYMMDD, format = "%Y") %>% 
    as.numeric()
data.to.plot.annual <- data.to.plot.local %>% 
  drop_na() %>% group_by(YYYY) %>%
    mutate( 
  # Change to max, min etc. as required.
          annual = sum(datumValue)
        ) 
```

# Plot Total Airfrost Days

```{r echo=FALSE}
data.to.plot.local <- VC55.weather.ss[VC55.weather.ss$datumAttribute == "airfrost.days", ]
data.to.plot.local$YYYY <- format(data.to.plot.local$YYYYMMDD, format = "%Y") %>% as.numeric()

data.to.plot.annual <- data.to.plot.local %>% drop_na() %>%
    group_by(YYYY) %>%
    mutate( 
          annual = sum(datumValue)
        ) 

ggfun(dat = data.to.plot.annual, nice.title = "Total Air Frost Days" ) 

#weather.plots.list$`max temp degC`

```

# Plot Maximum Temperature

```{r echo=FALSE}
data.to.plot.local <- VC55.weather.ss[VC55.weather.ss$datumAttribute == "tmax.degC", ]
data.to.plot.local$YYYY <- format(data.to.plot.local$YYYYMMDD, format = "%Y") %>% as.numeric()

data.to.plot.annual <- data.to.plot.local %>% drop_na() %>%
    group_by(YYYY) %>%
    mutate( 
          annual = max(datumValue)
        ) 

ggfun(dat = data.to.plot.annual, nice.title = "Maximum Temperature" ) 

```

# Plot Minimum Temperature

```{r echo=FALSE}
data.to.plot.local <- VC55.weather.ss[VC55.weather.ss$datumAttribute == "tmin.degC", ]
data.to.plot.local$YYYY <- format(data.to.plot.local$YYYYMMDD, format = "%Y") %>% as.numeric()

data.to.plot.annual <- data.to.plot.local %>% drop_na() %>%
    group_by(YYYY) %>%
    mutate( 
          annual = min(datumValue)
        ) 

ggfun(dat = data.to.plot.annual, nice.title = "Mainimum Temperature" ) 

```

# Plot Total Rainfail

```{r echo=FALSE}
data.to.plot.local <- VC55.weather.ss[VC55.weather.ss$datumAttribute == "rain.mm", ]
data.to.plot.local$YYYY <- format(data.to.plot.local$YYYYMMDD, format = "%Y") %>% as.numeric()

data.to.plot.annual <- data.to.plot.local %>% drop_na() %>%
    group_by(YYYY) %>%
    mutate( 
          annual = sum(datumValue)
        ) 

ggfun(dat = data.to.plot.annual, nice.title = "Annual Total Rainfall mm" ) 

```



# Conclusion

This vignette demonstrates the versatility of using data state in conjunction with GoG.

