---
title: \myTitleMainTitle
authors:
  - name: Dr Paul J. Palmer
    department: Wolfson School of Mechanical, Electrical and Manufacturing Engineering
    affiliation: Loughborough University
    location: Leicestershire VC55
    email: p.j.palmer@lboro.ac.uk
abstract: |
  \myAbstract 
bibliography: references.bib
biblio-style: unsrt
output:
  bookdown::pdf_book:
    base_format: rticles::arxiv_article
    extra_dependencies: ["flafter"]
    includes:
      in_header: "preamble.tex"  

---

```{r setDefaults,  include=TRUE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, results='asis'}
# Set code chunk options for all chunks
# These can be overridden at the chunk level, but setting global options 
# ensures consistency of chunk behaviour.
# To print a version of this document without code set echo = FALSE. 
#Include false will ignore the code chunk completely!
knitr::opts_chunk$set(include = TRUE, 
                      echo = TRUE, 
                      warning=FALSE, 
                      message=FALSE, 
                      error=FALSE, 
                     # fig.width = 3,
                    #  fig.height = 2,
                     # fig.width = 3,
                    out.width = '70%',
                      fig.align = "center",
                      results='asis')



```


# Introduction

\myTitleMainTitle

# Read Sutton Bonnington Data

Load the Tidyverse libraries and other helpers.
```{r }
# Load libraries
library(plyr)
library(tidyverse)
library(kableExtra)
library(gridExtra)
library(qqplotr)
library(readr)
```

Read the data. The absolute path helps to write code that is computer independent but it does use the Rstudio specific function to find the current project location. The use of `fs` to build the file path ensures compatibility across all operating systems. It also gives a chance to check that it is correct before loading the file.
```{r }
# Read data
absolute.path <- rprojroot::find_rstudio_root_file()
path.to.my.data <- fs::path( absolute.path,
                             "data-ext",
                             "suttonboningtondata", 
                            ext = "txt")
```

The `read_table` function does a good job of reading the data into columns recognising the partitioning with the use of spaces after the first five lines of text are skipped.
```{r }
suttonboningtondata <- read_table(
                        path.to.my.data, 
                        skip = 5) # skip 5 lines of text
```


Renaming the column names requires checking the results manually with `view(suttonboningtondata)` befor making a list of names.
```{r }
# Rename the columns to something useful
new.colnames <- c( "YYYY",
                   "mm",
                   "tmax.degC",
                   "tmin.degC",
                   "airfrost.days",
                   "rain.mm",
                   "sun.hours"
  )
colnames(suttonboningtondata) <- new.colnames
```

The text file headings were split across two rows so the first row of the data contains fragments which we can drop since we have included them in our column names manually.
```{r }
# Drop row 1
suttonboningtondata <- suttonboningtondata[-1,]
```

We can now remove the unwanted characters that remain in the data. Note the use of the double escape `\\*` to select the asterisk. The removal of unwanted special characters is often a problem in nascent data so it is best to get rid of them before they cause problems when code is executed.. 
```{r }
suttonboningtondata <- suttonboningtondata %>%
                        map_df( gsub,
                                pattern = "\\*",
                                replacement = "") %>%
                        map_df( gsub,
                                pattern = "---",
                                replacement = NA
                                )
```

The early months of the year are represented by a single digit so we need to pad them with a leading zero. The we can create a date in the ISO recognised format: `YYYY-MM-DD`. As these are monthly summaries we choose a date near the end of the month which allows us to convert from a character string to a date when we plot results.
```{r }
# Date is not properly encoded so build a date from the data
suttonboningtondata$mm <-   stringr::str_pad(suttonboningtondata$mm, width = 2, pad = "0")
 
 # Create a date at the end of the month.
suttonboningtondata$dd <- 28
 suttonboningtondata$YYYYMMDD <-   with( suttonboningtondata, 
                                     paste( YYYY, mm, 
                                            dd, sep= "-"   )    )
```

Each row of data needs a unique identifier which we call `datumEntity`.
```{r }
suttonboningtondata$datumEntity <- with(suttonboningtondata,
                                   paste("Sutton.Bonnington", YYYY, mm, sep = ":" )
                                   )
```

There are also some data attributes which were described in the five lines of text which apply to all rows of data. We manually add them here as they will be useful if we combine our data with similar sources from other locations.
```{r }
# manually name some values
suttonboningtondata$place <- "Sutton Bonnington"
suttonboningtondata$lattitude <- 52.833
suttonboningtondata$logitude <- 52.833
suttonboningtondata$logitude <- -1.250
suttonboningtondata$easting <- 450700
suttonboningtondata$northing <- 325900
suttonboningtondata$height.amsl.metre <- 48
```

The datum triple is the universal starting point for all data. Converting to this format allows us to combine data from multiple sources, as long as the `datumEntity` only ever refers to a unique observation. Note that this format makes no presumptions about attributes, but it does require all data to be represented as characters. There is no reason to keep any attribute with a value of `NA` as the row contains no information.
```{r }
suttonbonington.triple <- suttonboningtondata %>% 
                  map_df(as.character) %>% # Convert everything to character
                  pivot_longer(!datumEntity, # Use datumEntity as the key
                  names_to = "datumAttribute", # Save the name of the attribute
                  values_to = "datumValue" # Save the name of the value
                  ) %>% 
                  drop_na() # Drop all the NAs from datumValue
```

Finally we can save the data in a machine readable format for reuse. Effectively thi is `raw` data that is ready for analysis.
```{r }
dir.create( 
          fs::path( absolute.path,
            "data-ext" ),  
            showWarnings = FALSE,
            recursive = TRUE) # 

path.to.save<- fs::path( absolute.path
                               ,"data-ext","suttonboningtondata", ext = "rds")
saveRDS(suttonbonington.triple,path.to.save)


```

For the purpose of 
```{r}

```


```{r}

# Prepare data-ss
# First prepare the long data
VC55.weather.sl <- suttonbonington.triple %>%
                  pivot_wider(
  id_cols = datumEntity,
  names_from = datumAttribute,
  values_from = datumValue)

# But we are interesting in plotting the data by date so

  VC55.weather.ss <- VC55.weather.sl %>% 
    select("YYYYMMDD", 
           "tmax.degC",
           "tmin.degC",
           "airfrost.days", 
           "sun.hours" ,
           "rain.mm")     %>%
            pivot_longer(!YYYYMMDD,# Use datumEntity as the key
                  names_to = "datumAttribute", # Save the name of the attribute
                  values_to = "datumValue") # Save the value of the attribute

# Set the data types
VC55.weather.ss$YYYYMMDD <-  VC55.weather.ss$YYYYMMDD %>% as.Date
VC55.weather.ss$datumValue <- VC55.weather.ss$datumValue %>% as.numeric()

 
path.to.save.ss<- fs::path( absolute.path
                               ,"data-ss","VC55.weather.ss", ext = "rds")
saveRDS(VC55.weather.ss, path.to.save.ss)

```

```{r}

# Check plot

data.to.plot <- VC55.weather.ss

data.to.plot %>% drop_na() %>%
ggplot(aes(colour = datumAttribute, x= YYYYMMDD, y =datumValue, group = datumAttribute) ) + 
 # geom_point() +
  geom_smooth( se = FALSE, method = "loess", formula = "y ~ x") +
  facet_wrap(~ datumAttribute)
 #   geom_line() + facet_wrap(~ datumAttribute)



```

The plots are in different units so plot z-scores instead.

```{r}

# An improved summary


data.to.plot <- VC55.weather.ss
 data.to.plot %>% drop_na() %>%
  group_by(datumAttribute) %>% 
  mutate( value = datumValue,
          Z.score = (datumValue -mean(datumValue))/sd(datumValue)
        ) %>%
  ggplot( aes(colour = datumAttribute, 
              x= YYYYMMDD, y =Z.score, 
              group = datumAttribute) ) + 
              geom_smooth( se = FALSE, method = "loess", formula = "y ~ x") + 
 #   geom_point(data =data.to.plot[abs(data.to.plot$Z.score) >= 1.5,]) +
              scale_colour_viridis_d() 


  

```

This looks good but very small deviations from the mean.

```{r}

# An improved summary
my.dist <- "norm"

data.to.plot <- VC55.weather.ss
 data.to.plot %>% drop_na() %>%
  group_by(datumAttribute) %>% 
  mutate( value = datumValue,
          Z.score = (datumValue -mean(datumValue))/sd(datumValue)
        ) %>%
 #  ggplot( aes(colour = datumAttribute, 
 #              x= YYYYMMDD, y =Z.score, 
 #              group = datumAttribute) ) + 
 #              geom_smooth( se = FALSE, method = "loess", formula = "y ~ x") + 
 # #   geom_point(data =data.to.plot[abs(data.to.plot$Z.score) >= 1.5,]) +
 #              scale_colour_viridis_d() 



#ggplot(distribution.data, aes(x=centrality)) + stat_qq( sample = norm)

ggplot(mapping = aes( 
                      group = datumAttribute,
                      sample = Z.score)) +
 #   stat_qq_band(distribution = my.dist) +
    stat_qq_line(distribution = my.dist) +
   stat_qq_point(distribution = my.dist) + 
  scale_colour_viridis_d() +
  ggtitle("Z-score compared to normal distribution") + facet_wrap(~ datumAttribute)
  

```



# References {-}
<div id="refs"></div>
\let\cleardoublepage\clearpage
