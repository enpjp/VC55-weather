---
title: "Achieving Analytical Fluency With Complex Data"
subtitle: Refining the analytical process
titlerunning: Analytical Fluency
thanks: |
  The authors acknowledge EPSRC grant: EP/R513088/1 for funding the doctoral research on which this work is based.
authors:
- name: Paul J Palmer
  address: Department of YYY, University of XXX
  email: abc@def
- name: Michael Henshaw
  address: Department of ZZZ, University of WWW
  email: djf@wef
- name: Russell Lock
  address: Department of ZZZ, University of WWW
  email: djf@wef
keywords:
- key
- dictionary
- word
abstract: |
  Scientific analysis is formally presented as a rigid process typically
  Experiment; Analysis, Evaluation; and Conclusions.
  comprising: Review; Theory; Research question; Methodology;
  We do, however, question whether established methods for managing and
  analysing data are still appropriate for "Big Data", by which we mean
  data that is too big to be conveniently manipulated by manually
  intensive methods.
  is to justify why there is a need for new analytical techniques, given
  that the existing ones still work; the second is to show how an
  abstract perception of data impacts the analytical process.
  This presents two questions which we seek to address here: the first
  An example of the motivation for change is illustrated by the "Grammar
  of Graphics" (GoG) paradigm. GoG uses combinations of transforms to
  generate every possible graphic and tabulation from data presented in
  a suitable state allowing for a radical change in the analytic
  workflow, while still preserving the goals of scientific analysis.
  By framing this problem as one of transforming *data state*, we can
  mathematically describe general properties allowing the creation of
  re-usable code templates for data preparation. Coupled with "literate
  programming" techniques we show that this approach enables
  analytically fluent analysis of complex data.
bibliography: library.bib
biblio-style: spphys
authorrunning: P.J. Palmer, M. Henshaw, R.L. Lock
output:
  bookdown::pdf_book:
    base_format: rticles::springer_article
    includes:
      in_header: "jese.tex"
      keep_tex:  true  
---

# Introduction {#sec:introduction}

Scientific analysis is formally presented as a rigid process typically
comprising: Review; Theory; Research question; Methodology; Experiment;
Analysis, Evaluation; and Conclusions. This is a powerful framework that
has stood the test of time which is the underlying format of countless
academic publications and there is no reason to think that this format
will change.

If we look deeper, we can start to separate the actual process of
scientific discovery from the way in which it is reported. The
documentation of research and analysis that has already been completed,
is very different to an exploration where the outcomes are as yet
unknown. Nobel Prize winner @Szent-Gyorgyi1972 discussed this issue in
1972, long before the age of computers and "Big Data"[^2] introduced
computational and data intensive research methods [@Szent-Gyorgyi1972].
He described the Apollonian as following a formally prescribed research
path while a Dionysian explores the unknown. However, research and
reporting are necessarily linked and one should be a clear and true
interpretation of the other.

We argue here that this presumption the analytical process should follow
the final method of presentation is a hindrance to the adoption of new
analytical techniques. The term "reproducible research" has been coined
to describe this linkage, without defining how it may be achieved
leading to discussion on the topic by @Stodden2014
[@Lewis2018a; @Leeper2014; @Gentleman2007; @Peng2015; @Drummond2018] and
others.

Two questions arise from these discussions which we seek to address in
the following sections: the first is to justify why there is a need for
new analytical techniques, given that the existing ones still work? The
second is: how can changing our abstract perception of data beneficially
impact the analytical process?

Both the need and benefit are illustrated by the "Grammar of Graphics"
(GoG) paradigm [@Wilkinson2010]. GoG uses combinations of transforms to
generate every possible graphic and tabulation from data that is
presented in a suitable state. Thus, a researcher planning to use GoG
will focus on saving research data in suitable state, so she can make
many dynamic visualisations as work progresses, rather than waiting for
the end of the data collation phase.

This contrasts with the typical approach where difficulties with data
intensive analysis are often retrospectively blamed on imperfections
within the data that are seen as requiring "cleaning" before analysis
can begin. It is unfortunate that the popular spreadsheet encourages a
manually intensive formatting of data and the use of visualisation tools
that do not scale well with data size
[@Mack2018; @Bishop2013; @Powell2008].

Our alternative approach is consider data as observations of the real
world that are to be interpreted through analysis, rather than formatted
tables meeting arbitrary technical specifications.[^3] This also expands
on an idea hinted at in the GoG example above: there is no need to
rigidly define data format in advance since the real world is clearly
independent of the methods we choose to observe and record. If we
believe that the knowledge we seek is encapsulated within a set of
observations, then we can choose a viewpoint where we see data as
starting in an initial *inconvenient state*. It follows that we must
transform it into a *convenient state* for analysis. By constructing a
working theory supported with a mathematical description, we show that
the transformation of *data state* can be generalised to allow the
creation of re-usable code templates. To explain how re-framing the
problem of *imperfect data* in terms of *data state* can make such a
difference to the way in which we approach the analytical process, we
must first construct a methodology that takes into account the nature of
data.
